---
title: "Chapter 5 Statistical Rethinking"
output: html_document
date: "2022-10-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
devtools::install_github("mjskay/tidybayes.rethinking")
```

```{r}
library(rethinking)
library(tidyverse)
library(tidybayes)
library(tidybayes.rethinking)
library(dagitty)

```



### Easy Qestions

## Question 1

This question is pretty simple. Only 2 and 4 are models for multiple linear regressions. It includes $*B_xx_i$ and $B_zZ_I$


## Question 2
This would be the formula where A is animal diversity and P is plant diversity
$$
\begin {aligned}
P_i \sim \text{Normal}(\mu_i, \sigma)
\\
P_i = \alpha + \beta_A A_i + \beta_p P_i
\end{aligned}
$$


##Question 3
Assuming F is funding and L is the sizer of laboratory. The model would be this
$$
\begin {aligned}

P_i = \alpha + \beta_F F_i + \beta_L L_i
\end{aligned}
$$

Due to a PHD program being a time-investment, it physically cannot have a negative slope which means it should be a positive slope.

## Question 4.

This queestion asks which of these models of inferentially equivalent.


In this instance, 1,3,4,5 are inferentially equivalent.

Its easiest to explain why the second is not inferentially equivalent. 2 includes a slope for all 4 categories which makes it somewhat analytically useless and is thus it isn't capturing anything.

The first and third are inferentially equivalent on the same principle. They have three slopes and an intercept for the other.

The 4th provides the alphas for each of those which as Nico says (s/o nico), " we tell the model: just give me the expected values for each category. So this is inferentially equivalent: we can get at the same result." Which tells us the expected value.

According to pgs 154 and 155, the last one is also equivalent, but in all honesty. I am unsure of the reason for that.




### Medium Question

## Question 1

Alright lets invent the data
```{r}



d <- tibble(preal = rnorm(100),
            pfake = rnorm(100, preal),
            outcome = rnorm(100, -preal))
            
```

now lets run a simple regression for both the variables


```{r}
m1 <- quap(
  alist(
  outcome ~ dnorm(mu, sigma),
  mu <- a + B1 * preal, 
  a ~ dnorm(0, 0.2),
  B1 ~ dnorm(0,0.5),
  sigma ~ dexp(1)),
  data = d
)

m2 <- quap(
  alist(
    outcome ~ dnorm(mu, sigma),
    mu <- a + B2 * pfake,
    a ~ dnorm(0, 0.2),
    B2 ~ dnorm(0, 0.5),
    sigma ~ dexp(1)) ,
  data =d
    
)

## now lets do both

m3 <- quap(
  alist(
    outcome ~ dnorm(mu, sigma),
    mu <- a + B1 * preal + B2 * pfake,
    a ~ dnorm(0,0.2),
    B1 ~ dnorm(0, 0.5),
    B2 ~ dnorm(0, 0.5),
    sigma ~ dexp(1)),
  data=d
    
    
  )


```

```{r}
precis(m1)
precis(m2)
precis(m3)
```

As evidenced by the M3, the second one is revealed to effect the outcome little. However, in m2 there is a seeming connection, which is only revaled to be spurious with the third iteration.

## Question 2

This question asks whether or not we can note a masked relationship.

First we have to simulate the data.

```{r}
N <- 100

d2 <- tibble(
  p1 = rnorm(N),
  p2 = rnorm(N, p1),
  outcome = rnorm(N, p1-p2)
)
```

lets do some regressions then!
```{r}
## Bivariates!

BM1 <- quap(alist(
  outcome ~ dnorm(mu,sigma),
  mu <- a+ B1*p1,
  a ~ dnorm(0,0.2),
  B1 ~ dnorm(0,0.5),
  sigma ~ dexp(1)),
  data = d2
)

BM2 <- quap(alist(
  outcome ~ dnorm(mu,sigma),
  mu <- a+ B2*p2,
  a ~ dnorm(0,0.2),
  B2 ~ dnorm(0,0.5),
  sigma ~ dexp(1)),
  data = d2
)
```

Now lets do a multiple linear regression
```{r}
LM1 <- quap(alist(
  outcome ~ dnorm(mu, sigma),
  mu <- a + B1*p1 + B2 * p2,
  a ~ dnorm(0,0.2),
  B1 ~ dnorm(0,0.5),
  B2 ~ dnorm(0,0.5),
  sigma ~ dexp(1)),
  data = d2
)
```


```{r}
precis(BM1)
precis(BM2)
precis(LM1)
```

AS evidenced by BM1 and BM2, the relationship is somewhat masked. In the case of B1, the different is pretty stark. With the mean going fromn -.03,  to .76. In the instance of B2, the connection is slightly less maked insofar as it goes from -.53 to -.88. This indicates that a bivariate model is an ineffective means of understanding the relationship between these variables and the outcome.


## Question 3

This one asks why divorce data results in a higher marriage rate. This results happens because marriage rate isn't the unique variable insofar as it would track multiple marriagees for a single person. The way that you would resolve is to do a multiple regression that also includes remarriage rate after divorce.

## Question 4

Going to finish it after I get off work eta 7pm, even if you don't count it, I would love comments on it


## Question 5
This question asks how to deal with the multiple causation hypothesis regarding the relationship between price of gasoline vehicle and lower obesity rates.This would just be a 3 part multi-regession.G would represent price of Gasoline. W would be a variable which would represent amount walked. R would represent the amount eaten out at restaurants. You could also replace R with a variable C which refered to amount of food cooked at home.

$$
\begin {aligned}
\mu_i = \alpha + \beta_gG_i + \beta_wW_i + \beta_RR_i
\end {aligned}
$$



